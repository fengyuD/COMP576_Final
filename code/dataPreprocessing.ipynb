{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from scapy.utils import PcapReader, hexdump\n",
    "from scapy.layers.inet import IP, TCP, UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rawData\\\\BitTorrent.pcap': 'BitTorrent',\n",
      " 'rawData\\\\FTP.pcap': 'FTP',\n",
      " 'rawData\\\\Facetime.pcap': 'Facetime',\n",
      " 'rawData\\\\Gmail.pcap': 'Gmail',\n",
      " 'rawData\\\\MySQL.pcap': 'MySQL',\n",
      " 'rawData\\\\Outlook.pcap': 'Outlook',\n",
      " 'rawData\\\\SMB-1.pcap': 'SMB',\n",
      " 'rawData\\\\SMB-2.pcap': 'SMB',\n",
      " 'rawData\\\\Skype.pcap': 'Skype',\n",
      " 'rawData\\\\Weibo-1.pcap': 'Weibo',\n",
      " 'rawData\\\\Weibo-2.pcap': 'Weibo',\n",
      " 'rawData\\\\Weibo-3.pcap': 'Weibo',\n",
      " 'rawData\\\\Weibo-4.pcap': 'Weibo',\n",
      " 'rawData\\\\WorldOfWarcraft.pcap': 'WorldOfWarcraft'}\n"
     ]
    }
   ],
   "source": [
    "rowdata_folder = 'rawData'\n",
    "path_category_map = {}\n",
    "\n",
    "for filename in os.listdir(rowdata_folder):\n",
    "    category = filename.split('-')[0].split('.')[0]\n",
    "    filepath = os.path.join(rowdata_folder, filename)\n",
    "    path_category_map[filepath] = category\n",
    "\n",
    "pprint(path_category_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow integrate\n",
    "# {\n",
    "#     \"category1\": {\n",
    "#         \"five_tuple_1\": [\"payload1\", \"payload2\",...]\n",
    "#         \"five_tuple_2\": [\"payload1\", \"payload2\",...]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def extract_payload(pkt):\n",
    "    if UDP in pkt:\n",
    "        payload = pkt[UDP].payload\n",
    "    elif TCP in pkt:\n",
    "        payload = pkt[TCP].payload\n",
    "    pkt_payload_str = hexdump(payload, dump=True)\n",
    "    pkt_payload_str_list = pkt_payload_str.split('\\n')\n",
    "    pkt_payload_list = []\n",
    "    for line in pkt_payload_str_list:\n",
    "        if len(line.split('  ')) > 1:\n",
    "            pkt_payload_list.append(line.split('  ')[1])\n",
    "        \n",
    "    res = []\n",
    "    for line in pkt_payload_list:\n",
    "        res += line.split(\" \")\n",
    "    return ''.join(res)\n",
    "\n",
    "\n",
    "def generate_flow_payload(path_category_map):\n",
    "    flow_payload = {}\n",
    "    for (filepath, category) in tqdm(path_category_map.items()):\n",
    "        flow_payload.setdefault(category, {})\n",
    "        pcap_reader = PcapReader(filepath)\n",
    "        for pkt in pcap_reader:\n",
    "            if not (TCP in pkt or UDP in pkt):\n",
    "                continue\n",
    "\n",
    "            src_ip = pkt[IP].src\n",
    "            dst_ip = pkt[IP].dst\n",
    "            if TCP in pkt:\n",
    "                proto = 'tcp'\n",
    "                src_port = pkt[TCP].sport\n",
    "                dst_port = pkt[TCP].dport\n",
    "                payload = pkt[TCP].payload\n",
    "            if UDP in pkt:\n",
    "                proto = 'udp'\n",
    "                src_port = pkt[UDP].sport\n",
    "                dst_port = pkt[UDP].dport\n",
    "                payload = pkt[UDP].payload\n",
    "            \n",
    "            five_tuple_str = '|'.join([src_ip, dst_ip, str(src_port), str(dst_port), str(proto)])\n",
    "            processed_payload = extract_payload(pkt)\n",
    "            flow_payload[category].setdefault(five_tuple_str, []).append(processed_payload[:256])\n",
    "    return flow_payload\n",
    "\n",
    "def save_flow_payload(save_path, flow_payload):\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(flow_payload, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [1:49:45<00:00, 470.42s/it]   \n"
     ]
    }
   ],
   "source": [
    "save_path = 'flow_payload.json'\n",
    "flow_payload = generate_flow_payload(path_category_map)\n",
    "save_flow_payload(save_path, flow_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset generation\n",
    "bigram process\n",
    "tsv file\n",
    "train:test = 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitTorrent|15000\n",
      "Facetime|6000\n",
      "FTP|202034\n",
      "Gmail|17178\n",
      "MySQL|172114\n",
      "Outlook|14984\n",
      "Skype|12000\n",
      "SMB|77781\n",
      "Weibo|79810\n",
      "WorldOfWarcraft|15761\n"
     ]
    }
   ],
   "source": [
    "for key in flow_payload.keys():\n",
    "    print(f\"{key}|{len(flow_payload[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1.1.33.158|1.2.156.163|41319|443|tcp', ['170300057807090B36E9BEA600E6DFC59BD09C52754B56164C933C15D0ECA348229B36F55214F4FF5FFE2DFB595AB079887DFB96F4A4461252530B00F8A7B05CF3748FFED8BD8187A89A3184CFF195EC5EC28C4A45C6B5F7B891BA8E0821536D7EB140DD255AA7AA132217B4009CCDBA89D5923D2C2AFE118E90E1C921D8DE17'])\n"
     ]
    }
   ],
   "source": [
    "print((list(flow_payload['BitTorrent'].items())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bi_gram(payload):\n",
    "    res_list = []\n",
    "    i = 0\n",
    "    while(i<=len(payload)-4):\n",
    "        res_list.append(payload[i:i+4])\n",
    "        i += 2\n",
    "    bi_gram_payload = ' '.join(res_list[:300])\n",
    "    return bi_gram_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0|3000\n",
      "1|3000\n",
      "2|3000\n",
      "3|3000\n",
      "4|3000\n",
      "5|3000\n",
      "6|3000\n",
      "7|3000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "category_map_label = {\n",
    "    \"BitTorrent\": 0,\n",
    "    \"Facetime\": 1,\n",
    "    \"FTP\":2,\n",
    "    \"Gmail\": 3,\n",
    "    \"MySQL\": 4,\n",
    "    \"Outlook\": 3,\n",
    "    \"Skype\": 5,\n",
    "    \"SMB\": 2,\n",
    "    \"Weibo\": 6,\n",
    "    \"WorldOfWarcraft\": 7\n",
    "}\n",
    "\n",
    "flow_payload_path = \"./flow_payload.json\"\n",
    "raw_dataset = {}\n",
    "\n",
    "with open(flow_payload_path, 'r') as f:\n",
    "    flow_payload = json.load(f)\n",
    "    for category in flow_payload:\n",
    "        label = category_map_label[category]\n",
    "        for five_tuple in flow_payload[category]:\n",
    "            flow = \"\".join(flow_payload[category][five_tuple])\n",
    "            if (len(flow) > 50):\n",
    "                raw_dataset.setdefault(label, []).append(\"\".join(flow_payload[category][five_tuple]))\n",
    "    for label in raw_dataset:\n",
    "        random.shuffle(raw_dataset[label])\n",
    "        raw_dataset[label] = raw_dataset[label][:3000]\n",
    "\n",
    "for key in raw_dataset.keys():\n",
    "    print(f\"{key}|{len(raw_dataset[key])}\")\n",
    "\n",
    "bi_dataset = {}\n",
    "for label in raw_dataset:\n",
    "    bi_dataset[label] = [Bi_gram(payload) for payload in raw_dataset[label]]\n",
    "\n",
    "save_path = './bi_dataset.json'\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(bi_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19200\n",
      "4800\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "\n",
    "raw_data_path = './bi_dataset.json'\n",
    "dataset_folder = './dataset'\n",
    "\n",
    "with open(raw_data_path, 'r') as f:\n",
    "    data = []\n",
    "    bi_data = json.load(f)\n",
    "    for label in bi_data:\n",
    "        for flow in bi_data[label]:\n",
    "            data.append([label, flow])\n",
    "    random.shuffle(data)\n",
    "    train_data = data[:int(len(data)*0.8)]\n",
    "    test_data = data[int(len(data)*0.8):]\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(test_data)\n",
    "    print(len(train_data))\n",
    "    print(len(test_data))\n",
    "\n",
    "with open(dataset_folder+'/train.tsv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(['label', 'text_a'])\n",
    "    writer.writerows(train_data)\n",
    "\n",
    "with open(dataset_folder+'/test.tsv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(['label', 'text_a'])\n",
    "    writer.writerows(test_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
